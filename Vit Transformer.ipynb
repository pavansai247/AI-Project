{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.12.0\n",
      "  Using cached tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Using cached tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (24.3.25)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.11.0-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.28-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (18.1.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.23.5)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (4.25.3)\n",
      "Requirement already satisfied: setuptools in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.63.0-cp311-cp311-win_amd64.whl (3.9 MB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.12.0)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.43.0)\n",
      "Collecting ml-dtypes>=0.2.0\n",
      "  Using cached ml_dtypes-0.4.0-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Collecting scipy>=1.9\n",
      "  Using cached scipy-1.13.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.0.3)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.2.1)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow==2.12.0) (3.2.2)\n",
      "Installing collected packages: scipy, rsa, pyasn1-modules, opt-einsum, ml-dtypes, markdown, keras, idna, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, astunparse, absl-py, requests, jax, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'D:\\\\New folder\\\\Pavan Sai\\\\Data Science\\\\OpenCV\\\\Face Detection\\\\.facedetection\\\\Lib\\\\site-packages\\\\tensorboard\\\\backend\\\\application.py'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.0\n",
      "  Downloading torch-2.0.0-cp311-cp311-win_amd64.whl (172.3 MB)\n",
      "     -------------------------------------- 172.3/172.3 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from torch==2.0.0) (4.11.0)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ---------------------------------------- 5.7/5.7 MB 4.3 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.3/133.3 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     -------------------------------------- 536.2/536.2 kB 4.2 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.14.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 sympy-1.12 torch-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.image_paths = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                self.image_paths.append((os.path.join(cls_path, img_name), self.class_to_idx[cls_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(root_dir=r\"D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\archive\\train\", transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size * patch_size\n",
    "\n",
    "        self.patch_to_embedding = nn.Sequential(\n",
    "            nn.Conv2d(channels, dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b d h w -> b (h w) d')\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(dim, heads, mlp_dim, dropout),\n",
    "            depth\n",
    "        )\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.patch_to_embedding(x)\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "model = VisionTransformer(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=len(dataset.classes),\n",
    "    dim=256,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    mlp_dim=512,\n",
    "    channels=3,\n",
    "    dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7331, Accuracy: 50.82%\n",
      "Epoch [2/5], Loss: 0.7036, Accuracy: 50.68%\n",
      "Epoch [3/5], Loss: 0.7005, Accuracy: 49.91%\n",
      "Epoch [4/5], Loss: 0.7018, Accuracy: 50.05%\n",
      "Epoch [5/5], Loss: 0.7006, Accuracy: 49.14%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"vision_transformer_custom_dataset.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Female       0.50      1.00      0.67        40\n",
      "        Male       0.00      0.00      0.00        40\n",
      "\n",
      "    accuracy                           0.50        80\n",
      "   macro avg       0.25      0.50      0.33        80\n",
      "weighted avg       0.25      0.50      0.33        80\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to evaluate the model and print classification report and confusion matrix\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=dataset.classes)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have a test dataset\n",
    "test_dataset = CustomDataset(root_dir=r\"D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\archive\\test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"vision_transformer_custom_dataset.pth\"))\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class distribution: Counter({0: 1100, 1: 1100})\n",
      "Validation class distribution: Counter({0: 40, 1: 40})\n"
     ]
    }
   ],
   "source": [
    "## Check the dataset balanced or not \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "train_class_counts = Counter([label for _, label in dataset])\n",
    "print(\"Training class distribution:\", train_class_counts)\n",
    "\n",
    "val_class_counts = Counter([label for _, label in test_dataset])\n",
    "print(\"Validation class distribution:\", val_class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.8095, Training Accuracy: 53.64%\n",
      "Validation Accuracy: 65.45%\n",
      "Epoch [2/3], Loss: 0.4878, Training Accuracy: 76.53%\n",
      "Validation Accuracy: 80.68%\n",
      "Epoch [3/3], Loss: 0.3651, Training Accuracy: 84.94%\n",
      "Validation Accuracy: 86.82%\n",
      "Best Validation Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from timm import create_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.image_paths = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                self.image_paths.append((os.path.join(cls_path, img_name), self.class_to_idx[cls_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "dataset = CustomDataset(root_dir=r\"D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\archive\\train\", transform=transform)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check class distribution\n",
    "#labels = [label for _, label in train_dataset]\n",
    "#class_counts = Counter(labels)\n",
    "#print(\"Class distribution:\", class_counts)\n",
    "#plt.bar(class_counts.keys(), class_counts.values())\n",
    "#plt.xlabel('Class')\n",
    "#plt.ylabel('Number of samples')\n",
    "#plt.title('Class distribution in training dataset')\n",
    "#plt.show()\n",
    "\n",
    "# Load pre-trained ViT model and modify for your dataset\n",
    "num_classes = len(dataset.classes)\n",
    "model = create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Optional: Freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop with diagnostics\n",
    "num_epochs = 3\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save the best model\n",
    "    #if val_accuracy > best_accuracy:\n",
    "        #best_accuracy = val_accuracy\n",
    "        #torch.save(model.state_dict(), \"best_vit_model.pth\")\n",
    "        #print(\"Model saved!\")\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Female       0.90      0.88      0.89        40\n",
      "        Male       0.88      0.90      0.89        40\n",
      "\n",
      "    accuracy                           0.89        80\n",
      "   macro avg       0.89      0.89      0.89        80\n",
      "weighted avg       0.89      0.89      0.89        80\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwgAAAJwCAYAAAAtA0YPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABE60lEQVR4nO3dfXzN9f/H8eeZizM2u3K1KeZaNEJfoVzmWiSjyNfXUPqq6cLoYiUZ1foqdCVUclXoW6FSWS5CpL7IGGnZ0KS5iN+wsWH7/P4wH5/T0I62cw7nce/2ud2c9/l8Pu/XZ32/Oq89P+/zsRmGYQgAAAAAJPm4uwAAAAAAnoMGAQAAAICJBgEAAACAiQYBAAAAgIkGAQAAAICJBgEAAACAiQYBAAAAgIkGAQAAAICJBgEAAACAiQYBAC5i165d6ty5swIDA2Wz2bRkyZIiPf/evXtls9k0e/bsIj3v1axdu3Zq166du8sAAK9HgwDAY6Wmpurf//63atasKV9fXwUEBOi2227Ta6+9plOnThXr3FFRUUpKStILL7ygefPm6R//+EexzudKgwcPls1mU0BAwEV/jrt27ZLNZpPNZtMrr7zi9Pl///13jRs3TomJiUVQLQDA1Uq6uwAAuJgvvvhCd999t+x2uwYNGqSIiAidPn1a69at0+OPP64dO3bo7bffLpa5T506pQ0bNuiZZ57RiBEjimWO8PBwnTp1SqVKlSqW8/+VkiVL6uTJk/r88891zz33OLz3wQcfyNfXV9nZ2Vd07t9//11xcXGqXr26GjduXOjjvv766yuaDwBQtGgQAHicPXv2qH///goPD9eqVasUFhZmvhcdHa2UlBR98cUXxTb/4cOHJUlBQUHFNofNZpOvr2+xnf+v2O123XbbbVqwYEGBBmH+/Pm644479Mknn7iklpMnT6ps2bIqXbq0S+YDAFwetxgB8DgTJ05UZmamZs6c6dAcnFe7dm09+uij5uuzZ89qwoQJqlWrlux2u6pXr66nn35aOTk5DsdVr15dPXr00Lp163TLLbfI19dXNWvW1Ny5c819xo0bp/DwcEnS448/LpvNpurVq0s6d2vO+T9bjRs3TjabzWFs+fLlatWqlYKCguTv76969erp6aefNt+/1BqEVatWqXXr1vLz81NQUJB69eqlnTt3XnS+lJQUDR48WEFBQQoMDNSQIUN08uTJS/9g/2TAgAH66quvlJGRYY5t3LhRu3bt0oABAwrsf/ToUY0ePVoNGzaUv7+/AgIC1K1bN23dutXcZ/Xq1WrWrJkkaciQIeatSuevs127doqIiNDmzZvVpk0blS1b1vy5/HkNQlRUlHx9fQtcf5cuXRQcHKzff/+90NcKACg8GgQAHufzzz9XzZo1deuttxZq//vvv19jx45V06ZNNWXKFLVt21bx8fHq379/gX1TUlLUt29fderUSZMmTVJwcLAGDx6sHTt2SJIiIyM1ZcoUSdK9996refPm6dVXX3Wq/h07dqhHjx7KycnR+PHjNWnSJN15551av379ZY9bsWKFunTpokOHDmncuHGKiYnRd999p9tuu0179+4tsP8999yjEydOKD4+Xvfcc49mz56tuLi4QtcZGRkpm82mRYsWmWPz58/XDTfcoKZNmxbYf/fu3VqyZIl69OihyZMn6/HHH1dSUpLatm1rflivX7++xo8fL0l64IEHNG/ePM2bN09t2rQxz3PkyBF169ZNjRs31quvvqr27dtftL7XXntNFStWVFRUlHJzcyVJM2bM0Ndff6033nhDVapUKfS1AgCcYACABzl27JghyejVq1eh9k9MTDQkGffff7/D+OjRow1JxqpVq8yx8PBwQ5Kxdu1ac+zQoUOG3W43Ro0aZY7t2bPHkGS8/PLLDueMiooywsPDC9Tw3HPPGda/TqdMmWJIMg4fPnzJus/PMWvWLHOscePGRqVKlYwjR46YY1u3bjV8fHyMQYMGFZhv6NChDufs3bu3Ub58+UvOab0OPz8/wzAMo2/fvkaHDh0MwzCM3NxcIzQ01IiLi7vozyA7O9vIzc0tcB12u90YP368ObZx48YC13Ze27ZtDUnG9OnTL/pe27ZtHcYSEhIMScbzzz9v7N692/D39zfuuuuuv7xGAMCVI0EA4FGOHz8uSSpXrlyh9v/yyy8lSTExMQ7jo0aNkqQCaxUaNGig1q1bm68rVqyoevXqaffu3Vdc85+dX7vw6aefKi8vr1DHpKenKzExUYMHD1ZISIg53qhRI3Xq1Mm8Tqvhw4c7vG7durWOHDli/gwLY8CAAVq9erUOHDigVatW6cCBAxe9vUg6t27Bx+fcfzZyc3N15MgR8/apH3/8sdBz2u12DRkypFD7du7cWf/+9781fvx4RUZGytfXVzNmzCj0XAAA59EgAPAoAQEBkqQTJ04Uav9ff/1VPj4+ql27tsN4aGiogoKC9OuvvzqMV6tWrcA5goOD9X//939XWHFB/fr102233ab7779flStXVv/+/fXf//73ss3C+Trr1atX4L369evrjz/+UFZWlsP4n68lODhYkpy6lu7du6tcuXL68MMP9cEHH6hZs2YFfpbn5eXlacqUKapTp47sdrsqVKigihUratu2bTp27Fih57zuuuucWpD8yiuvKCQkRImJiXr99ddVqVKlQh8LAHAeDQIAjxIQEKAqVapo+/btTh3350XCl1KiRImLjhuGccVznL8//rwyZcpo7dq1WrFihf71r39p27Zt6tevnzp16lRg37/j71zLeXa7XZGRkZozZ44WL158yfRAkl588UXFxMSoTZs2ev/995WQkKDly5frxhtvLHRSIp37+Thjy5YtOnTokCQpKSnJqWMBAM6jQQDgcXr06KHU1FRt2LDhL/cNDw9XXl6edu3a5TB+8OBBZWRkmN9IVBSCg4MdvvHnvD+nFJLk4+OjDh06aPLkyfrpp5/0wgsvaNWqVfrmm28ueu7zdSYnJxd47+eff1aFChXk5+f39y7gEgYMGKAtW7boxIkTF13Yfd7HH3+s9u3ba+bMmerfv786d+6sjh07FviZFLZZK4ysrCwNGTJEDRo00AMPPKCJEydq48aNRXZ+AEBBNAgAPM4TTzwhPz8/3X///Tp48GCB91NTU/Xaa69JOneLjKQC3zQ0efJkSdIdd9xRZHXVqlVLx44d07Zt28yx9PR0LV682GG/o0ePFjj2/APD/vzVq+eFhYWpcePGmjNnjsMH7u3bt+vrr782r7M4tG/fXhMmTNCbb76p0NDQS+5XokSJAunERx99pP379zuMnW9kLtZMOevJJ59UWlqa5syZo8mTJ6t69eqKioq65M8RAPD38aA0AB6nVq1amj9/vvr166f69es7PEn5u+++00cffaTBgwdLkm666SZFRUXp7bffVkZGhtq2bav//e9/mjNnju66665LfoXmlejfv7+efPJJ9e7dW4888ohOnjypadOmqW7dug6LdMePH6+1a9fqjjvuUHh4uA4dOqS33npL119/vVq1anXJ87/88svq1q2bWrZsqfvuu0+nTp3SG2+8ocDAQI0bN67IruPPfHx8NGbMmL/cr0ePHho/fryGDBmiW2+9VUlJSfrggw9Us2ZNh/1q1aqloKAgTZ8+XeXKlZOfn5+aN2+uGjVqOFXXqlWr9NZbb+m5554zv3Z11qxZateunZ599llNnDjRqfMBAAqHBAGAR7rzzju1bds29e3bV59++qmio6P11FNPae/evZo0aZJef/11c993331XcXFx2rhxox577DGtWrVKsbGxWrhwYZHWVL58eS1evFhly5bVE088oTlz5ig+Pl49e/YsUHu1atX03nvvKTo6WlOnTlWbNm20atUqBQYGXvL8HTt21LJly1S+fHmNHTtWr7zyilq0aKH169c7/eG6ODz99NMaNWqUEhIS9Oijj+rHH3/UF198oapVqzrsV6pUKc2ZM0clSpTQ8OHDde+992rNmjVOzXXixAkNHTpUTZo00TPPPGOOt27dWo8++qgmTZqk77//vkiuCwDgyGY4s5oNAAAAwDWNBAEAAACAiQYBAAAAgIkGAQAAAICJBgEAAACAiQYBAAAAgIkGAQAAAICJBgEAAACA6Zp8knKZFk+6uwQAKFIHV8e7uwQAKFIBvp77e+oyTUa4bK5TW9502VyF5bn/ZgAAAAC43DWZIAAAAABXzObdv0P37qsHAAAA4IAEAQAAALCy2dxdgVuRIAAAAAAwkSAAAAAAVqxBAAAAAIBzSBAAAAAAK9YgAAAAAMA5JAgAAACAFWsQAAAAAOAcEgQAAADAijUIAAAAADzdtGnT1KhRIwUEBCggIEAtW7bUV199Zb7frl072Ww2h2348OFOz0OCAAAAAFh56BqE66+/Xi+99JLq1KkjwzA0Z84c9erVS1u2bNGNN94oSRo2bJjGjx9vHlO2bFmn56FBAAAAAK4CPXv2dHj9wgsvaNq0afr+++/NBqFs2bIKDQ39W/N4ZnsEAAAAeIGcnBwdP37cYcvJyfnL43Jzc7Vw4UJlZWWpZcuW5vgHH3ygChUqKCIiQrGxsTp58qTTNdEgAAAAAFY2m8u2+Ph4BQYGOmzx8fGXLC0pKUn+/v6y2+0aPny4Fi9erAYNGkiSBgwYoPfff1/ffPONYmNjNW/ePA0cOND5yzcMw7jiH56HKtPiSXeXAABF6uDqS//HAgCuRgG+nvt76jItn3LZXBmr4wokBna7XXa7/aL7nz59WmlpaTp27Jg+/vhjvfvuu1qzZo3ZJFitWrVKHTp0UEpKimrVqlXomliDAAAAAFi5cJHy5ZqBiyldurRq164tSbr55pu1ceNGvfbaa5oxY0aBfZs3by5JTjcIntu6AQAAALisvLy8S65ZSExMlCSFhYU5dU4SBAAAAMDKQx+UFhsbq27duqlatWo6ceKE5s+fr9WrVyshIUGpqamaP3++unfvrvLly2vbtm0aOXKk2rRpo0aNGjk1Dw0CAAAAcBU4dOiQBg0apPT0dAUGBqpRo0ZKSEhQp06dtG/fPq1YsUKvvvqqsrKyVLVqVfXp00djxoxxeh4aBAAAAMDKQx+UNnPmzEu+V7VqVa1Zs6ZI5vHMqwcAAADgFiQIAAAAgJWHrkFwFRIEAAAAACYSBAAAAMDKQ9cguIp3Xz0AAAAAByQIAAAAgBUJAgAAAACcQ4IAAAAAWPnwLUYAAAAAIIkEAQAAAHDEGgQAAAAAOIcGAQAAAICJW4wAAAAAKxuLlAEAAABAEgkCAAAA4IhFygAAAABwDgkCAAAAYMUaBAAAAAA4hwQBAAAAsGINAgAAAACcQ4IAAAAAWLEGAQAAAADOIUEAAAAArFiDAAAAAADnkCAAAAAAVqxBAAAAAIBzSBAAAAAAK9YgAAAAAMA5JAgAAACAFWsQAAAAAOAcEgQAAADAijUIAAAAAHAODQIAAAAAE7cYAQAAAFbcYgQAAAAA55AgAAAAAFZ8zSkAAAAAnEOCAAAAAFixBgEAAAAAziFBAAAAAKxYgwAAAAAA55AgAAAAAFasQQAAAACAc0gQAAAAACvWIAAAAADAOSQIAAAAgIWNBAEAAAAAziFBAAAAACxIEAAAAAAgHwkCAAAAYOXdAQIJAgAAAIALaBAAAAAAmLjFCAAAALBgkTIAAAAA5CNBAAAAACxIEAAAAAAgHwkCAAAAYEGCAAAAAAD5SBAAAAAACxIEAAAAAMhHggAAAABYeXeAQIIAAAAA4AISBAAAAMCCNQgAAAAAkI8EAQAAALAgQQAAAACAfCQIAAAAgAUJAgAAAADkI0EAAAAALEgQAAAAACAfCQIAAABg5d0BAgkCAAAAgAtoEAAAAACYaBAAAAAAC5vN5rLNGdOmTVOjRo0UEBCggIAAtWzZUl999ZX5fnZ2tqKjo1W+fHn5+/urT58+OnjwoNPXT4MAAAAAXAWuv/56vfTSS9q8ebM2bdqk22+/Xb169dKOHTskSSNHjtTnn3+ujz76SGvWrNHvv/+uyMhIp+exGYZhFHXx7lamxZPuLgEAitTB1fHuLgEAilSAr+f+nrrikA9dNtfhWf3+1vEhISF6+eWX1bdvX1WsWFHz589X3759JUk///yz6tevrw0bNqhFixaFPqfn/psBAAAArnE5OTk6fvy4w5aTk/OXx+Xm5mrhwoXKyspSy5YttXnzZp05c0YdO3Y097nhhhtUrVo1bdiwwamaaBAAAAAAC1euQYiPj1dgYKDDFh9/6dQ4KSlJ/v7+stvtGj58uBYvXqwGDRrowIEDKl26tIKCghz2r1y5sg4cOODU9fMcBAAAAMBNYmNjFRMT4zBmt9svuX+9evWUmJioY8eO6eOPP1ZUVJTWrFlTpDXRIAAAAABWLnxQmt1uv2xD8GelS5dW7dq1JUk333yzNm7cqNdee039+vXT6dOnlZGR4ZAiHDx4UKGhoU7VxC1GAAAAwFUqLy9POTk5uvnmm1WqVCmtXLnSfC85OVlpaWlq2bKlU+ckQQAAAAAsnH0+gavExsaqW7duqlatmk6cOKH58+dr9erVSkhIUGBgoO677z7FxMQoJCREAQEBevjhh9WyZUunvsFIokEAAAAArgqHDh3SoEGDlJ6ersDAQDVq1EgJCQnq1KmTJGnKlCny8fFRnz59lJOToy5duuitt95yeh6egwAAVwGegwDgWuPJz0EIHfaxy+Y68E5fl81VWJ77bwYAAACAy3GLEQAAAGDhqWsQXIUEAQAAAICJBAEAAACwIEEAAAAAgHwe0yBkZGTo3XffVWxsrI4ePSpJ+vHHH7V//343VwYAAACvYnPh5oE84hajbdu2qWPHjgoMDNTevXs1bNgwhYSEaNGiRUpLS9PcuXPdXSIAAADgFTwiQYiJidHgwYO1a9cu+fr6muPdu3fX2rVr3VgZAAAA4F08IkHYuHGjZsyYUWD8uuuu04EDB9xQEQAAALwVi5Q9gN1u1/HjxwuM//LLL6pYsaIbKgIAAAC8k0c0CHfeeafGjx+vM2fOSDrXtaWlpenJJ59Unz593FwdAAAAvInNZnPZ5ok8okGYNGmSMjMzValSJZ06dUpt27ZV7dq1Va5cOb3wwgvuLg8AAADwGh6xBiEwMFDLly/XunXrtG3bNmVmZqpp06bq2LGju0sDAACAl/HU3+y7ikc0COe1atVKrVq1cncZAAAAgNdyW4Pw+uuvF3rfRx55pBgrAQAAACy8O0BwX4MwZcqUQu1ns9loEAAAAAAXcVuDsGfPHndNDQAAAFySt69B8IhvMQIAAADgGTxmkfJvv/2mzz77TGlpaTp9+rTDe5MnT3ZTVQAAAPA23p4geESDsHLlSt15552qWbOmfv75Z0VERGjv3r0yDENNmzZ1d3kAAACA1/CIW4xiY2M1evRoJSUlydfXV5988on27duntm3b6u6773Z3eQAAAPAi3v4kZY9IEHbu3KkFCxZIkkqWLKlTp07J399f48ePV69evfTggw+6uUJ4s2GRLTQssoXCw4IlSTt3H9SL763U1xuSJUkJbz2gNk1rORzzzqLv9cjExS6vFQCuxNvT3tQ706c6jIVXr6GPP/3STRUBcCePaBD8/PzMdQdhYWFKTU3VjTfeKEn6448/3FkaoP2HjunZqV8p5bc/ZJNNA++4WR9NHKQWg17Xzj0HJUkzl/ygCW9/bR5zMvuMu8oFgCtSs1ZtTX37PfN1yRIe8REBcAtP/c2+q3jE//tbtGihdevWqX79+urevbtGjRqlpKQkLVq0SC1atHB3efByX67b6fB63PQEDevdQrdEVDMbhFPZZ3TwaKY7ygOAIlGiZElVqFDR3WUA8AAe0SBMnjxZmZnnPlzFxcUpMzNTH374oerUqcM3GMGj+PjY1Of2RvIrU1o/JP1qjvfr0lj9uzbRwSMn9OW6nYp/b6VO5ZAiALh67Pv1V3Xr2EalS9vV8KbGGvHISIWGVXF3WYB7eHeA4BkNQs2aNc0/+/n5afr06YU+NicnRzk5OQ5jRt5Z2Xw84tJwjbixVqhWv/OQfEuXVOap0+r35Fz9vPeQJOnDhESlHchQ+h/H1bB2qJ6P7q664RXV/6l5bq4aAArnxoaN9NyEFxVevYb+OHxY78yYqmFDBmrhJ5/Lz8/P3eUBcDGP+xSdmZmpvLw8h7GAgIBL7h8fH6+4uDiHsRLX3apS17cqlvrgnX759bCaD3pNgX6+6n17Q70z9h51fnCGft57SO99+j9zvx2pB5T+xwktm/qAalwXoj37j7qxagAonNtatTH/XKduPUU0bKSe3TpoRcJX6hXZ142VAe7h7WsQPOJrTvfs2aM77rhDfn5+CgwMVHBwsIKDgxUUFKTg4ODLHhsbG6tjx445bCWrsG4BRevM2Vzt/u2ItiTv19hpy5SUkq7ofhdvQjfuSJMk1bq+gitLBIAiUy4gQNXCq2vfvjR3lwLADTwiQRg4cKAMw9B7772nypUrO9W12e122e12hzFuL0Jx87HZZC9d4qLv3VT33D27B44cd2VJAFBkTp7M0v59+1ThjjvdXQoAN/CIT9Jbt27V5s2bVa9ePXeXAhQw/sGuStiQrH0HM1SurF39OjdWm6Y11fOx91TjuhD169xECd/9rCPHT6ph7VBNfLSnvv1xt7anHHB36QBQKK9OmqjWbdspLOw6HT58SG9Pe0M+JXzUpdsd7i4NcAtvv8XIIxqEZs2aad++fTQI8EgVg/0187l7FFo+QMcys7U9NV09H3tPq/63S9dXCtTtzWprRP/b5OdbWr8dOqYlq5P00nur3F02ABTaoYMHNOap0TqWkaHg4BDd1KSpZs1bqOCQEHeXBsANbIZhGO4uIjU1VcOHD9fAgQMVERGhUqVKObzfqFEjp85XpsWTRVkeALjdwdXx7i4BAIpUgK9HLIW9qNqjv3LZXCmvdHPZXIXlEQnC4cOHlZqaqiFDhphjNptNhmHIZrMpNzfXjdUBAAAA3sMjGoShQ4eqSZMmWrBggdOLlAEAAICi5O2fRT2iQfj111/12WefqXbt2u4uBQAAAPBqHnHz1+23366tW7e6uwwAAABANpvrNk/kEQlCz549NXLkSCUlJalhw4YFFinfeSffwwwAAAC4gkc0CMOHD5ckjR8/vsB7LFIGAACAK7EGwQPk5eW5uwQAAAAA8pAGwSo7O1u+vr7uLgMAAABeyssDBM9YpJybm6sJEybouuuuk7+/v3bv3i1JevbZZzVz5kw3VwcAAAB4D49oEF544QXNnj1bEydOVOnSpc3xiIgIvfvuu26sDAAAAN7Gx8fmss0TeUSDMHfuXL399tv65z//qRIlSpjjN910k37++Wc3VgYAAAB4F49Yg7B///6LPiQtLy9PZ86ccUNFAAAA8FasQfAADRo00Lfffltg/OOPP1aTJk3cUBEAAADgnTwiQRg7dqyioqK0f/9+5eXladGiRUpOTtbcuXO1dOlSd5cHAAAAL+Ltz0Fwa4Kwe/duGYahXr166fPPP9eKFSvk5+ensWPHaufOnfr888/VqVMnd5YIAAAAeBW3Jgh16tRRenq6KlWqpNatWyskJERJSUmqXLmyO8sCAAAAvJZbGwTDMBxef/XVV8rKynJTNQAAAACLlD1ikfJ5f24YAAAAALiWWxMEm81WYBGIty8KAQAAgHt5++dRt99iNHjwYNntdklSdna2hg8fLj8/P4f9Fi1a5I7yAAAAAK/j1gYhKirK4fXAgQPdVAkAAABwDgmCG82aNcud0wMAAAD4E494UBoAAADgKbw8QPCsbzECAAAA4F4kCAAAAICFt69BIEEAAAAAYCJBAAAAACy8PEAgQQAAAABwAQkCAAAAYMEaBAAAAADIR4IAAAAAWHh5gECCAAAAAOACEgQAAADAgjUIAAAAAJCPBAEAAACw8PIAgQQBAAAAwAU0CAAAAABM3GIEAAAAWLBIGQAAAADykSAAAAAAFl4eIJAgAAAAALiABgEAAACwsNlsLtucER8fr2bNmqlcuXKqVKmS7rrrLiUnJzvs065duwJzDB8+3Kl5aBAAAACAq8CaNWsUHR2t77//XsuXL9eZM2fUuXNnZWVlOew3bNgwpaenm9vEiROdmoc1CAAAAICFp65BWLZsmcPr2bNnq1KlStq8ebPatGljjpctW1ahoaFXPA8JAgAAAOAmOTk5On78uMOWk5NTqGOPHTsmSQoJCXEY/+CDD1ShQgVFREQoNjZWJ0+edKomGgQAAADAwpVrEOLj4xUYGOiwxcfH/2WNeXl5euyxx3TbbbcpIiLCHB8wYIDef/99ffPNN4qNjdW8efM0cOBAp66fW4wAAAAAN4mNjVVMTIzDmN1u/8vjoqOjtX37dq1bt85h/IEHHjD/3LBhQ4WFhalDhw5KTU1VrVq1ClUTDQIAAABg4co1CHa7vVANgdWIESO0dOlSrV27Vtdff/1l923evLkkKSUlhQYBAAAAuJYYhqGHH35Yixcv1urVq1WjRo2/PCYxMVGSFBYWVuh5aBAAAAAAC2efT+Aq0dHRmj9/vj799FOVK1dOBw4ckCQFBgaqTJkySk1N1fz589W9e3eVL19e27Zt08iRI9WmTRs1atSo0PPQIAAAAABXgWnTpkk69zA0q1mzZmnw4MEqXbq0VqxYoVdffVVZWVmqWrWq+vTpozFjxjg1Dw0CAAAAYOGpCYJhGJd9v2rVqlqzZs3fnoevOQUAAABgIkEAAAAALDw0QHAZEgQAAAAAJhoEAAAAACZuMQIAAAAsPHWRsquQIAAAAAAwkSAAAAAAFl4eIJAgAAAAALiABAEAAACwYA0CAAAAAOQjQQAAAAAsvDxAIEEAAAAAcAEJAgAAAGDh4+URAgkCAAAAABMJAgAAAGDh5QECCQIAAACAC0gQAAAAAAuegwAAAAAA+UgQAAAAAAsf7w4QSBAAAAAAXECCAAAAAFiwBgEAAAAA8pEgAAAAABZeHiCQIAAAAAC4gAYBAAAAgIlbjAAAAAALm7z7HiMSBAAAAAAmEgQAAADAggelAQAAAEA+EgQAAADAggelAQAAAEA+EgQAAADAwssDBBIEAAAAABeQIAAAAAAWPl4eIZAgAAAAADCRIAAAAAAWXh4gkCAAAAAAuIAEAQAAALDgOQgAAAAAkI8EAQAAALDw8gCBBAEAAADABSQIAAAAgAXPQQAAAACAfDQIAAAAAEzcYgQAAABYePcNRiQIAAAAACxIEAAAAAALHpQGAAAAAPlIEAAAAAALH+8OEEgQAAAAAFxAggAAAABYsAYBAAAAAPKRIAAAAAAWXh4gkCAAAAAAuIAEAQAAALBgDQIAAAAA5CNBAAAAACx4DgIAAAAA5CNBAAAAACxYgwAAAAAA+UgQAAAAAAvvzg9IEAAAAABYkCAAAAAAFj6sQQAAAACAc2gQAAAAAJiuqEH49ttvNXDgQLVs2VL79++XJM2bN0/r1q0r0uIAAAAAV7PZXLd5IqcbhE8++URdunRRmTJltGXLFuXk5EiSjh07phdffLHICwQAAADgOk43CM8//7ymT5+ud955R6VKlTLHb7vtNv34449FWhwAAADgajabzWWbJ3K6QUhOTlabNm0KjAcGBiojI6MoagIAAADgJk43CKGhoUpJSSkwvm7dOtWsWbNIigIAAADchTUITho2bJgeffRR/fDDD7LZbPr999/1wQcfaPTo0XrwwQeLo0YAAAAALuL0g9Keeuop5eXlqUOHDjp58qTatGkju92u0aNH6+GHHy6OGgEAAACX8fYHpTndINhsNj3zzDN6/PHHlZKSoszMTDVo0ED+/v7FUR8AAAAAF7riB6WVLl1aDRo00C233EJzAAAAgGuGp65BiI+PV7NmzVSuXDlVqlRJd911l5KTkx32yc7OVnR0tMqXLy9/f3/16dNHBw8edGoepxOE9u3bX/YrmVatWuXsKQEAAAD8hTVr1ig6OlrNmjXT2bNn9fTTT6tz58766aef5OfnJ0kaOXKkvvjiC3300UcKDAzUiBEjFBkZqfXr1xd6HqcbhMaNGzu8PnPmjBITE7V9+3ZFRUU5ezoAAADAo3jq8wmWLVvm8Hr27NmqVKmSNm/erDZt2ujYsWOaOXOm5s+fr9tvv12SNGvWLNWvX1/ff/+9WrRoUah5nG4QpkyZctHxcePGKTMz09nTAQAAAF4rJydHOTk5DmN2u112u/0vjz127JgkKSQkRJK0efNmnTlzRh07djT3ueGGG1StWjVt2LCh+BqESxk4cKBuueUWvfLKK0V1yiv2f+v+4+4SAKBIBTcb4e4SAKBIndryprtLuKQrXqR7BeLj4xUXF+cw9txzz2ncuHGXPS4vL0+PPfaYbrvtNkVEREiSDhw4oNKlSysoKMhh38qVK+vAgQOFrqnIGoQNGzbI19e3qE4HAAAAXPNiY2MVExPjMFaY9CA6Olrbt2/XunXrirwmpxuEyMhIh9eGYSg9PV2bNm3Ss88+W2SFAQAAAO7gyjUIhb2dyGrEiBFaunSp1q5dq+uvv94cDw0N1enTp5WRkeGQIhw8eFChoaGFPr/TDUJgYKDDax8fH9WrV0/jx49X586dnT0dAAAAgEIwDEMPP/ywFi9erNWrV6tGjRoO7998880qVaqUVq5cqT59+kiSkpOTlZaWppYtWxZ6HqcahNzcXA0ZMkQNGzZUcHCwM4cCAAAAVwUfz/wSI0VHR2v+/Pn69NNPVa5cOXNdQWBgoMqUKaPAwEDdd999iomJUUhIiAICAvTwww+rZcuWhV6gLDm5BqNEiRLq3LmzMjIynLoYAAAAAH/PtGnTdOzYMbVr105hYWHm9uGHH5r7TJkyRT169FCfPn3Upk0bhYaGatGiRU7N4/QtRhEREdq9e3eBSAMAAABA8TEM4y/38fX11dSpUzV16tQrnsfpb3F6/vnnNXr0aC1dulTp6ek6fvy4wwYAAABczXxsrts8UaEThPHjx2vUqFHq3r27JOnOO+90WOFtGIZsNptyc3OLvkoAAAAALlHoBiEuLk7Dhw/XN998U5z1AAAAAG7lyq859USFbhDO3/PUtm3bYisGAAAAgHs5tUjZ27spAAAAXPs8dW2AqzjVINStW/cvm4SjR4/+rYIAAAAAuI9TDUJcXFyBJykDAAAA1xJvv2nGqQahf//+qlSpUnHVAgAAAMDNCt0gsP4AAAAA3sDHyz/3FvpBaYV5chsAAACAq1uhE4S8vLzirAMAAADwCIX+Dfo1ytuvHwAAAICFU4uUAQAAgGudly9BIEEAAAAAcAEJAgAAAGDBtxgBAAAAQD4SBAAAAMDCywMEEgQAAAAAF5AgAAAAABY+JAgAAAAAcA4NAgAAAAATtxgBAAAAFnzNKQAAAADkI0EAAAAALLw8QCBBAAAAAHABCQIAAABgwdecAgAAAEA+EgQAAADAwibvjhBIEAAAAACYSBAAAAAAC9YgAAAAAEA+EgQAAADAggQBAAAAAPKRIAAAAAAWNi9/lDIJAgAAAAATCQIAAABgwRoEAAAAAMhHggAAAABYePkSBBIEAAAAABfQIAAAAAAwcYsRAAAAYOHj5fcYkSAAAAAAMJEgAAAAABZ8zSkAAAAA5CNBAAAAACy8fAkCCQIAAACAC0gQAAAAAAsfeXeEQIIAAAAAwESCAAAAAFiwBgEAAAAA8pEgAAAAABY8BwEAAAAA8pEgAAAAABY+Xr4IgQQBAAAAgIkEAQAAALDw8gCBBAEAAADABSQIAAAAgAVrEAAAAAAgHwkCAAAAYOHlAQIJAgAAAIALaBAAAAAAmLjFCAAAALDw9t+ge/v1AwAAALAgQQAAAAAsbF6+SpkEAQAAAICJBAEAAACw8O78gAQBAAAAgAUJAgAAAGDhwxoEAAAAADiHBAEAAACw8O78gAQBAAAAgAUJAgAAAGDh5UsQSBAAAAAAXECCAAAAAFjwJGUAAAAAHm/t2rXq2bOnqlSpIpvNpiVLlji8P3jwYNlsNoeta9euTs9DggAAAABYeOpv0LOysnTTTTdp6NChioyMvOg+Xbt21axZs8zXdrvd6XloEAAAAAA3ycnJUU5OjsOY3W6/6Af7bt26qVu3bpc9n91uV2ho6N+qyVMbJAAAAMAt/nybTnFu8fHxCgwMdNji4+OvuPbVq1erUqVKqlevnh588EEdOXLE6XOQIAAAAABuEhsbq5iYGIexK7ktSDp3e1FkZKRq1Kih1NRUPf300+rWrZs2bNigEiVKFPo8NAgAAACAm1zqdqIr0b9/f/PPDRs2VKNGjVSrVi2tXr1aHTp0KPR5uMUIAAAAsLC5cCtONWvWVIUKFZSSkuLUcTQIAAAAwDXot99+05EjRxQWFubUcdxiBAAAAFh46oPSMjMzHdKAPXv2KDExUSEhIQoJCVFcXJz69Omj0NBQpaam6oknnlDt2rXVpUsXp+ahQQAAAACuAps2bVL79u3N1+cXN0dFRWnatGnatm2b5syZo4yMDFWpUkWdO3fWhAkTnF7jQIMAAAAAWHjqPfjt2rWTYRiXfD8hIaFI5vHU6wcAAADgBiQIAAAAgIWnrkFwFRIEAAAAACYSBAAAAMDCu/MDEgQAAAAAFiQIAAAAgIWXL0EgQQAAAABwAQkCAAAAYOHj5asQSBAAAAAAmEgQAAAAAAvWIAAAAABAPhIEAAAAwMLGGgQAAAAAOIcEAQAAALBgDQIAAAAA5KNBAAAAAGDiFiMAAADAggelAQAAAEA+EgQAAADAgkXKAAAAAJCPBAEAAACwIEEAAAAAgHwkCAAAAICFjW8xAgAAAIBzSBAAAAAACx/vDhBIEAAAAABcQIIAAAAAWLAGAQAAAADykSAAAAAAFjwHAQAAAADyeVSDkJKSooSEBJ06dUqSZBiGmysCAACAt7G58B9P5BENwpEjR9SxY0fVrVtX3bt3V3p6uiTpvvvu06hRo9xcHQAAAOA9PKJBGDlypEqWLKm0tDSVLVvWHO/Xr5+WLVvmxsoAAADgbXxsrts8kUcsUv7666+VkJCg66+/3mG8Tp06+vXXX91UFQAAAOB9PCJByMrKckgOzjt69KjsdrsbKgIAAAC8k0c0CK1bt9bcuXPN1zabTXl5eZo4caLat2/vxsoAAADgbbx9kbJH3GI0ceJEdejQQZs2bdLp06f1xBNPaMeOHTp69KjWr1/v7vIAAAAAr+ERCUJERIR++eUXtWrVSr169VJWVpYiIyO1ZcsW1apVy93lAQAAwIvYbK7bPJFHJAiSFBgYqGeeecbdZQBOm/nO23r91Un658BBeiKW/w0D8HzD7m6lYX1bK7xKiCRp5+4DevHtr/T1+p/MfZo3qqFx0T3UrGF15ebmadsv+9XzoanKzjnjrrIBuIjbGoRt27YVet9GjRoVYyXAlduetE0ff7RQdevWc3cpAFBo+w9m6Nk3PlVK2mHZZNPAns310ZQH1KL/S9q5+4CaN6qhT998SK/M+lox//lIZ3Pz1KjudcrL4wGm8A4e+ot9l3Fbg9C4cWPZbLa/fFqyzWZTbm6ui6oCCu9kVpZin3xcz8U9r3dmTHN3OQBQaF+u3e7wetzUzzXs7la6pVEN7dx9QBNHReqthav1yqzl5j67fj3k6jIBuInbGoQ9e/a4a2qgSLz4/Hi1adNWLVreSoMA4Krl42NTn05N5VemtH7YtkcVg/11S6MaWvjVJn0zO0Y1rq+gX/Ye1Lg3P9d3ibvdXS7gEj6eujjARdzWIISHhxfJeXJycpSTk+MwZpSw8/wEFKuvvvxCO3f+pPkffuzuUgDgitxYu4pWzxkl39IllXkqR/1GvaOfdx/QLQ2rS5Ke+Xd3xU5ZrG3Jv+mfPW7RlzMe1s13v6jUtMPuLRxAsfOYRcqS9NNPPyktLU2nT592GL/zzjsveUx8fLzi4uIcxp559jmNGTuuOEoEdCA9XRNfekEz3nmPRhTAVeuXvQfVvH+8Av3LqHfHJnpn/L/U+f7X5ONz7jenMz9Zp3mffS9J2pr8m9rdUk9RvVpq7BufubNswCW8Oz/wkAZh9+7d6t27t5KSkhzWJdjy453LrUGIjY1VTEyMw5hRgg9tKD4//bRDR48cUf+7I82x3Nxcbd60UQsXfKCNW5JUokQJN1YIAH/tzNlc7d73hyRpy859uvnGaoq+t5257mDn7gMO+yfvOaCqocEurxOA63lEg/Doo4+qRo0aWrlypWrUqKH//e9/OnLkiEaNGqVXXnnlssfa7QVvJ8o+W5zVwts1b9FCHy/53GHsuWdiVb1mTQ25bxjNAYCrko/NJnvpkvr19yP6/VCG6lav5PB+7fBKDl+DClzTvDxC8IgGYcOGDVq1apUqVKggHx8f+fj4qFWrVoqPj9cjjzyiLVu2uLtEwOTn5686deo6jJUpW1ZBgUEFxgHAE41/+E4lrN+hfen/p3J+vurX7R9q84866vnQW5KkKXNWaMzwO5T0y35tTf5NA3s2V73qlTXg8ZlurhyAK3hEg5Cbm6ty5cpJkipUqKDff/9d9erVU3h4uJKTk91cHQAA15aKIf6aOWGQQisE6Fhmtrbv2q+eD72lVT/8LEl6c/5q+dpLaeKoPgoOLKukX/arx4Nvas9vf7i5csA1bF4eIXhEgxAREaGtW7eqRo0aat68uSZOnKjSpUvr7bffVs2aNd1dHvCXZs6e5+4SAKDQHoyb/5f7vDJrucNzEAB4D49oEMaMGaOsrCxJUlxcnHr27KnWrVurfPnyWrhwoZurAwAAgDfx8scgeEaD0KVLF/PPderU0c8//6yjR48qODjY/CYjAAAAAMXPrQ3C0KFDC7Xfe++9V8yVAAAAAOd4+6+n3dogzJ49W+Hh4WrSpIn57AMAAAAA7uPWBuHBBx/UggULtGfPHg0ZMkQDBw5USEiIO0sCAACAt/PyCMHHnZNPnTpV6enpeuKJJ/T555+ratWquueee5SQkECiAAAAALiBWxsE6dyTkO+9914tX75cP/30k2688UY99NBDql69ujIzM91dHgAAAOBVPOJbjM7z8fGRzWaTYRjKzc11dzkAAADwQt7+oDS3Jwg5OTlasGCBOnXqpLp16yopKUlvvvmm0tLS5O/v7+7yAAAAAK/i1gThoYce0sKFC1W1alUNHTpUCxYsUIUKFdxZEgAAALyctz+Gy2a4cTWwj4+PqlWrpiZNmlz2gWiLFi1y6rzZZ/9uZQDgWYKbjXB3CQBQpE5tedPdJVzS5r3HXTbXzdUDXDZXYbk1QRg0aBBPSgYAAIBH8fZPp25/UBoAAAAAz+FR32IEAAAAuJ2XRwhu/xYjAAAAAJ6DBAEAAACw4DkIAAAAAJCPBAEAAACw8PYv2SRBAAAAAGAiQQAAAAAsvDxAIEEAAAAAcAEJAgAAAGDl5RECCQIAAAAAEwkCAAAAYMFzEAAAAAB4vLVr16pnz56qUqWKbDablixZ4vC+YRgaO3aswsLCVKZMGXXs2FG7du1yeh4aBAAAAOAqkJWVpZtuuklTp0696PsTJ07U66+/runTp+uHH36Qn5+funTpouzsbKfm4RYjAAAAwMJTH5TWrVs3devW7aLvGYahV199VWPGjFGvXr0kSXPnzlXlypW1ZMkS9e/fv9DzkCAAAAAAbpKTk6Pjx487bDk5OU6fZ8+ePTpw4IA6duxojgUGBqp58+basGGDU+eiQQAAAAAsbC7c4uPjFRgY6LDFx8c7XfOBAwckSZUrV3YYr1y5svleYXGLEQAAAOAmsbGxiomJcRiz2+1uquYcGgQAAADAyoVrEOx2e5E0BKGhoZKkgwcPKiwszBw/ePCgGjdu7NS5uMUIAAAAuMrVqFFDoaGhWrlypTl2/Phx/fDDD2rZsqVT5yJBAAAAACw89UFpmZmZSklJMV/v2bNHiYmJCgkJUbVq1fTYY4/p+eefV506dVSjRg09++yzqlKliu666y6n5qFBAAAAAK4CmzZtUvv27c3X59cuREVFafbs2XriiSeUlZWlBx54QBkZGWrVqpWWLVsmX19fp+axGYZhFGnlHiD7rLsrAICiFdxshLtLAIAidWrLm+4u4ZJ++j3LZXM1qOLnsrkKizUIAAAAAEzcYgQAAABYeOYKBNchQQAAAABgIkEAAAAArLw8QiBBAAAAAGAiQQAAAAAsPPU5CK5CggAAAADARIIAAAAAWNi8O0AgQQAAAABwAQ0CAAAAABO3GAEAAAAWXn6HEQkCAAAAgAtIEAAAAAArL48QSBAAAAAAmEgQAAAAAAselAYAAAAA+UgQAAAAAAselAYAAAAA+UgQAAAAAAsvDxBIEAAAAABcQIIAAAAAWHl5hECCAAAAAMBEggAAAABY8BwEAAAAAMhHggAAAABY8BwEAAAAAMhHggAAAABYeHmAQIIAAAAA4AISBAAAAMDKyyMEEgQAAAAAJhoEAAAAACZuMQIAAAAseFAaAAAAAOQjQQAAAAAseFAaAAAAAOQjQQAAAAAsvDxAIEEAAAAAcAEJAgAAAGDBGgQAAAAAyEeCAAAAADjw7giBBAEAAACAiQQBAAAAsGANAgAAAADkI0EAAAAALLw8QCBBAAAAAHABCQIAAABgwRoEAAAAAMhHggAAAABY2Lx8FQIJAgAAAAATDQIAAAAAE7cYAQAAAFbefYcRCQIAAACAC0gQAAAAAAsvDxBIEAAAAABcQIIAAAAAWPCgNAAAAADIR4IAAAAAWPCgNAAAAADIR4IAAAAAWHl3gECCAAAAAOACEgQAAADAwssDBBIEAAAAABeQIAAAAAAWPAcBAAAAAPKRIAAAAAAWPAcBAAAAAPKRIAAAAAAWrEEAAAAAgHw0CAAAAABMNAgAAAAATDQIAAAAAEwsUgYAAAAsWKQMAAAAAPloEAAAAAALmwv/cca4ceNks9kcthtuuKHIr59bjAAAAICrxI033qgVK1aYr0uWLPqP8zQIAAAAgIUnr0EoWbKkQkNDi3UObjECAAAA3CQnJ0fHjx932HJyci65/65du1SlShXVrFlT//znP5WWllbkNdEgAAAAABY2F27x8fEKDAx02OLj4y9aV/PmzTV79mwtW7ZM06ZN0549e9S6dWudOHGiaK/fMAyjSM/oAbLPursCAChawc1GuLsEAChSp7a86e4SLulEdp7L5iptO1MgMbDb7bLb7X95bEZGhsLDwzV58mTdd999RVYTaxAAAAAAKxeuQShsM3AxQUFBqlu3rlJSUoq0Jm4xAgAAAK5CmZmZSk1NVVhYWJGelwYBAAAAsPDU5yCMHj1aa9as0d69e/Xdd9+pd+/eKlGihO69994ivX5uMQIAAACuAr/99pvuvfdeHTlyRBUrVlSrVq30/fffq2LFikU6Dw0CAAAAYOGpz0FYuHChS+bhFiMAAAAAJhIEAAAAwMJDAwSXIUEAAAAAYCJBAAAAAKy8PEIgQQAAAABgokEAAAAAYOIWIwAAAMDC2QeYXWtIEAAAAACYSBAAAAAAC099UJqrkCAAAAAAMNkMwzDcXQRwNcrJyVF8fLxiY2Nlt9vdXQ4A/G38vQZAokEArtjx48cVGBioY8eOKSAgwN3lAMDfxt9rACRuMQIAAABgQYMAAAAAwESDAAAAAMBEgwBcIbvdrueee46FfACuGfy9BkBikTIAAAAACxIEAAAAACYaBAAAAAAmGgQAAAAAJhoEwA2qV6+uV1991d1lAMBf2rt3r2w2mxITE91dCgAXoUHANW/w4MGy2WwFtpSUFHeXBgDF4vzfe8OHDy/wXnR0tGw2mwYPHuz6wgBcFWgQ4BW6du2q9PR0h61GjRruLgsAik3VqlW1cOFCnTp1yhzLzs7W/PnzVa1aNTdWBsDT0SDAK9jtdoWGhjpsJUqU0KeffqqmTZvK19dXNWvWVFxcnM6ePWseZ7PZNGPGDPXo0UNly5ZV/fr1tWHDBqWkpKhdu3by8/PTrbfeqtTUVPOY1NRU9erVS5UrV5a/v7+aNWumFStWXLa+jIwM3X///apYsaICAgJ0++23a+vWrcX28wBw7WvatKmqVq2qRYsWmWOLFi1StWrV1KRJE3Ns2bJlatWqlYKCglS+fHn16NHD4e+0i9m+fbu6desmf39/Va5cWf/617/0xx9/FNu1AHAtGgR4rW+//VaDBg3So48+qp9++kkzZszQ7Nmz9cILLzjsN2HCBA0aNEiJiYm64YYbNGDAAP373/9WbGysNm3aJMMwNGLECHP/zMxMde/eXStXrtSWLVvUtWtX9ezZU2lpaZes5e6779ahQ4f01VdfafPmzWratKk6dOigo0ePFtv1A7j2DR06VLNmzTJfv/feexoyZIjDPllZWYqJidGmTZu0cuVK+fj4qHfv3srLy7voOTMyMnT77berSZMm2rRpk5YtW6aDBw/qnnvuKdZrAeBCBnCNi4qKMkqUKGH4+fmZW9++fY0OHToYL774osO+8+bNM8LCwszXkowxY8aYrzds2GBIMmbOnGmOLViwwPD19b1sDTfeeKPxxhtvmK/Dw8ONKVOmGIZhGN9++60REBBgZGdnOxxTq1YtY8aMGU5fLwBERUUZvXr1Mg4dOmTY7XZj7969xt69ew1fX1/j8OHDRq9evYyoqKiLHnv48GFDkpGUlGQYhmHs2bPHkGRs2bLFMAzDmDBhgtG5c2eHY/bt22dIMpKTk4vzsgC4SEm3dieAi7Rv317Tpk0zX/v5+alRo0Zav369Q2KQm5ur7OxsnTx5UmXLlpUkNWrUyHy/cuXKkqSGDRs6jGVnZ+v48eMKCAhQZmamxo0bpy+++ELp6ek6e/asTp06dckEYevWrcrMzFT58uUdxk+dOvWXMT8AXE7FihV1xx13aPbs2TIMQ3fccYcqVKjgsM+uXbs0duxY/fDDD/rjjz/M5CAtLU0REREFzrl161Z988038vf3L/Beamqq6tatWzwXA8BlaBDgFfz8/FS7dm2HsczMTMXFxSkyMrLA/r6+vuafS5UqZf7ZZrNdcuz8f1RHjx6t5cuX65VXXlHt2rVVpkwZ9e3bV6dPn75obZmZmQoLC9Pq1asLvBcUFFS4CwSASxg6dKh5G+TUqVMLvN+zZ0+Fh4frnXfeUZUqVZSXl6eIiIjL/p3Vs2dP/ec//ynwXlhYWNEWD8AtaBDgtZo2bark5OQCjcPftX79eg0ePFi9e/eWdO4/pnv37r1sHQcOHFDJkiVVvXr1Iq0FALp27arTp0/LZrOpS5cuDu8dOXJEycnJeuedd9S6dWtJ0rp16y57vqZNm+qTTz5R9erVVbIkHyOAaxGLlOG1xo4dq7lz5youLk47duzQzp07tXDhQo0ZM+ZvnbdOnTpatGiREhMTtXXrVg0YMOCSi/0kqWPHjmrZsqXuuusuff3119q7d6++++47PfPMM9q0adPfqgUASpQooZ07d+qnn35SiRIlHN4LDg5W+fLl9fbbbyslJUWrVq1STEzMZc8XHR2to0eP6t5779XGjRuVmpqqhIQEDRkyRLm5ucV5KQBchAYBXqtLly5aunSpvv76azVr1kwtWrTQlClTFB4e/rfOO3nyZAUHB+vWW29Vz5491aVLFzVt2vSS+9tsNn355Zdq06aNhgwZorp166p///769ddfzTUPAPB3BAQEKCAgoMC4j4+PFi5cqM2bNysiIkIjR47Uyy+/fNlzValSRevXr1dubq46d+6shg0b6rHHHlNQUJB8fPhYAVwLbIZhGO4uAgAAAIBnoNUHAAAAYKJBAAAAAGCiQQAAAABgokEAAAAAYKJBAAAAAGCiQQAAAABgokEAAAAAYKJBAAAAAGCiQQAADzN48GDddddd5ut27drpsccec3kdq1evls1mU0ZGhsvnBgC4Dw0CABTS4MGDZbPZZLPZVLp0adWuXVvjx4/X2bNni3XeRYsWacKECYXalw/1AIC/q6S7CwCAq0nXrl01a9Ys5eTk6Msvv1R0dLRKlSql2NhYh/1Onz6t0qVLF8mcISEhRXIeAAAKgwQBAJxgt9sVGhqq8PBwPfjgg+rYsaM+++wz87agF154QVWqVFG9evUkSfv27dM999yjoKAghYSEqFevXtq7d695vtzcXMXExCgoKEjly5fXE088IcMwHOb88y1GOTk5evLJJ1W1alXZ7XbVrl1bM2fO1N69e9W+fXtJUnBwsGw2mwYPHixJysvLU3x8vGrUqKEyZcropptu0scff+wwz5dffqm6deuqTJkyat++vUOdAADvQYMAAH9DmTJldPr0aUnSypUrlZycrOXLl2vp0qU6c+aMunTponLlyunbb7/V+vXr5e/vr65du5rHTJo0SbNnz9Z7772ndevW6ejRo1q8ePFl5xw0aJAWLFig119/XTt37tSMGTPk7++vqlWr6pNPPpEkJScnKz09Xa+99pokKT4+XnPnztX06dO1Y8cOjRw5UgMHDtSaNWsknWtkIiMj1bNnTyUmJur+++/XU089VVw/NgCAB+MWIwC4AoZhaOXKlUpISNDDDz+sw4cPy8/PT++++655a9H777+vvLw8vfvuu7LZbJKkWbNmKSgoSKtXr1bnzp316quvKjY2VpGRkZKk6dOnKyEh4ZLz/vLLL/rvf/+r5cuXq2PHjpKkmjVrmu+fvx2pUqVKCgoKknQucXjxxRe1YsUKtWzZ0jxm3bp1mjFjhtq2batp06apVq1amjRpkiSpXr16SkpK0n/+858i/KkBAK4GNAgA4ISlS5fK399fZ86cUV5engYMGKBx48YpOjpaDRs2dFh3sHXrVqWkpKhcuXIO58jOzlZqaqqOHTum9PR0NW/e3HyvZMmS+sc//lHgNqPzEhMTVaJECbVt27bQNaekpOjkyZPq1KmTw/jp06fVpEkTSdLOnTsd6pBkNhMAAO9CgwAATmjfvr2mTZum0qVLq0qVKipZ8sJfo35+fg77ZmZm6uabb9YHH3xQ4DwVK1a8ovnLlCnj9DGZmZmSpC+++ELXXXedw3t2u/2K6gAAXLtoEADACX5+fqpdu3ah9m3atKk+/PBDVapUSQEBARfdJywsTD/88IPatGkjSTp79qw2b96spk2bXnT/hg0bKi8vT2vWrDFvMbI6n2Dk5uaaYw0aNJDdbldaWtolk4f69evrs88+cxj7/vvv//oiAQDXHBYpA0Ax+ec//6kKFSqoV69e+vbbb7Vnzx6tXr1ajzzyiH777TdJ0qOPPqqXXnpJS5Ys0c8//6yHHnross8wqF69uqKiojR06FAtWbLEPOd///tfSVJ4eLhsNpuWLl2qw4cPKzMzU+XKldPo0aM1cuRIzZkzR6mpqfrxxx/1xhtvaM6cOZKk4cOHa9euXXr88ceVnJys+fPna/bs2cX9IwIAeCAaBAAoJmXLltXatWtVrVo1RUZGqn79+rrvvvuUnZ1tJgqjRo3Sv/71L0VFRally5YqV66cevfufdnzTps2TX379tVDDz2kG264QcOGDVNWVpYk6brrrlNcXJyeeuopVa5cWSNGjJAkTZgwQc8++6zi4+NVv359de3aVV988YVq1KghSapWrZo++eQTLVmyRDfddJOmT5+uF198sRh/OgAAT2UzLrUSDgAAAIDXIUEAAAAAYKJBAAAAAGCiQQAAAABgokEAAAAAYKJBAAAAAGCiQQAAAABgokEAAAAAYKJBAAAAAGCiQQAAAABgokEAAAAAYKJBAAAAAGD6fyYyqEn8oCasAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test1_dataset = CustomDataset(root_dir=r\"D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\archive\\test\", transform=transform)\n",
    "\n",
    "test_loader = DataLoader(test1_dataset, batch_size=32, shuffle=False)\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=dataset.classes)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Load the best model and evaluate it\n",
    "\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from timm import create_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        self.image_paths = []\n",
    "        for cls_name in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls_name)\n",
    "            for img_name in os.listdir(cls_path):\n",
    "                self.image_paths.append((os.path.join(cls_path, img_name), self.class_to_idx[cls_name]))\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(root_dir=r\"D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\Images_aug\\Training\", transform=transform)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check class distribution\n",
    "#labels = [label for _, label in train_dataset]\n",
    "#class_counts = Counter(labels)\n",
    "#print(\"Class distribution:\", class_counts)\n",
    "#plt.bar(class_counts.keys(), class_counts.values())\n",
    "#plt.xlabel('Class')\n",
    "#plt.ylabel('Number of samples')\n",
    "#plt.title('Class distribution in training dataset')\n",
    "#plt.show()\n",
    "\n",
    "# Load pre-trained ViT model and modify for your dataset\n",
    "num_classes = len(dataset.classes)\n",
    "model = create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Optional: Freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIT Transformer on dataset2 around 3000 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 3.6118, Training Accuracy: 91.54%\n",
      "Validation Accuracy: 92.88%\n",
      "Model saved!\n",
      "Epoch [2/4], Loss: 2.2760, Training Accuracy: 94.10%\n",
      "Validation Accuracy: 93.84%\n",
      "Model saved!\n",
      "Epoch [3/4], Loss: 1.9897, Training Accuracy: 95.18%\n",
      "Validation Accuracy: 94.40%\n",
      "Model saved!\n",
      "Epoch [4/4], Loss: 1.7932, Training Accuracy: 95.80%\n",
      "Validation Accuracy: 93.44%\n",
      "Best Validation Accuracy: 94.40%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-1)\n",
    "\n",
    "# Training loop with diagnostics\n",
    "num_epochs = 4\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_vit_model_dataset2.pth\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 93.44%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      female       0.79      0.95      0.86      1100\n",
      "        male       0.94      0.74      0.83      1100\n",
      "\n",
      "    accuracy                           0.85      2200\n",
      "   macro avg       0.86      0.85      0.84      2200\n",
      "weighted avg       0.86      0.85      0.84      2200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test2_dataset = CustomDataset(root_dir=r\"D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\archive\\train\", transform=transform)\n",
    "\n",
    "test1_loader = DataLoader(test2_dataset, batch_size=32, shuffle=False)\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=dataset.classes)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Load the best model and evaluate it\n",
    "\n",
    "evaluate_model(model, test1_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "\n",
    "# Load the model\n",
    "model = create_model('vit_base_patch16_224', pretrained=False)\n",
    "num_classes = 2  # Update this to match the number of classes\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(\"best_vit_model_dataset2.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to predict the class of an image\n",
    "def predict_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# Example usage\n",
    "image_path = \"\"\n",
    "predicted_class = predict_image(image_path)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.14-cp311-cp311-win_amd64.whl (50.8 MB)\n",
      "     ---------------------------------------- 50.8/50.8 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from mediapipe) (0.4.28)\n",
      "Collecting jaxlib\n",
      "  Downloading jaxlib-0.4.28-cp311-cp311-win_amd64.whl (51.5 MB)\n",
      "     ---------------------------------------- 51.5/51.5 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from mediapipe) (3.9.0)\n",
      "Requirement already satisfied: numpy in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from mediapipe) (1.23.5)\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.9.0.80-cp37-abi3-win_amd64.whl (45.3 MB)\n",
      "     ---------------------------------------- 45.3/45.3 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Collecting sounddevice>=0.4.4\n",
      "  Downloading sounddevice-0.4.7-py3-none-win_amd64.whl (200 kB)\n",
      "     -------------------------------------- 200.1/200.1 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting CFFI>=1.0\n",
      "  Downloading cffi-1.16.0-cp311-cp311-win_amd64.whl (181 kB)\n",
      "     -------------------------------------- 181.5/181.5 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from jax->mediapipe) (1.13.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "     ---------------------------------------- 117.6/117.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in d:\\new folder\\pavan sai\\data science\\opencv\\face detection\\.facedetection\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: pycparser, opencv-contrib-python, jaxlib, CFFI, sounddevice, mediapipe\n",
      "Successfully installed CFFI-1.16.0 jaxlib-0.4.28 mediapipe-0.10.14 opencv-contrib-python-4.9.0.80 pycparser-2.22 sounddevice-0.4.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 110.jpg, Bounding Box: x=1182, y=1740, w=2370, h=2370\n",
      "Image: 111.jpg, Bounding Box: x=461, y=1211, w=995, h=1091\n",
      "Image: 112.jpg, Bounding Box: x=547, y=846, w=1230, h=1230\n",
      "Image: 113.jpg, Bounding Box: x=796, y=1636, w=1669, h=1668\n",
      "Image: 114.jpg, Bounding Box: x=601, y=1287, w=2547, h=2547\n",
      "Image: 115.jpg, Bounding Box: x=859, y=2374, w=2766, h=2767\n",
      "Image: 116.jpg, Bounding Box: x=710, y=1307, w=2977, h=2977\n",
      "Image: 117.jpg, Bounding Box: x=608, y=885, w=1270, h=1270\n",
      "Image: 118.jpg, Bounding Box: x=212, y=1237, w=1668, h=1668\n",
      "Image: 119.jpg, Bounding Box: x=429, y=941, w=1807, h=1807\n",
      "Image: 120.jpg, Bounding Box: x=270, y=674, w=702, h=702\n",
      "Image: 121.jpg, Bounding Box: x=559, y=486, w=1023, h=1023\n",
      "Image: 122.jpg, Bounding Box: x=109, y=413, w=1277, h=1277\n",
      "Image: 123.jpg, Bounding Box: x=939, y=1163, w=1932, h=1932\n",
      "Image: 124.jpg, Bounding Box: x=1685, y=2618, w=1644, h=1644\n",
      "Image: 125.jpg, Bounding Box: x=158, y=247, w=424, h=424\n",
      "Image: 126.jpg, Bounding Box: x=234, y=465, w=531, h=531\n",
      "Image: 127.jpg, Bounding Box: x=557, y=2129, w=1366, h=1366\n",
      "Image: 128.jpg, Bounding Box: x=386, y=821, w=1178, h=1178\n",
      "Image: 129.jpg, Bounding Box: x=1057, y=1452, w=1841, h=1840\n",
      "Image: 130.jpg, Bounding Box: x=389, y=740, w=962, h=962\n",
      "Image: 131.jpg, Bounding Box: x=431, y=953, w=1045, h=1045\n",
      "Image: 132.jpg, Bounding Box: x=462, y=724, w=1195, h=1195\n",
      "Image: 133.jpg, Bounding Box: x=280, y=406, w=488, h=488\n",
      "Image: 134.jpg, Bounding Box: x=990, y=1401, w=2494, h=2494\n",
      "Image: 135.jpg, Bounding Box: x=175, y=1163, w=1425, h=1430\n",
      "Image: 136.jpg, Bounding Box: x=246, y=676, w=1356, h=1356\n",
      "Image: 137.jpg, Bounding Box: x=359, y=979, w=1719, h=1720\n",
      "Image: 138.jpg, Bounding Box: x=687, y=974, w=1102, h=1102\n",
      "Image: 139.jpg, Bounding Box: x=494, y=933, w=1423, h=1423\n",
      "Image: 140.jpg, Bounding Box: x=657, y=1131, w=1434, h=1434\n",
      "Image: 141.jpg, Bounding Box: x=1281, y=2303, w=2098, h=2098\n",
      "Image: 142.jpg, Bounding Box: x=1140, y=2143, w=1306, h=1306\n",
      "Image: 143.jpg, Bounding Box: x=164, y=307, w=352, h=352\n",
      "Image: 144.jpg, Bounding Box: x=419, y=733, w=1471, h=1471\n",
      "Image: 145.jpg, Bounding Box: x=708, y=1151, w=2024, h=2024\n",
      "Image: 146.jpg, Bounding Box: x=485, y=575, w=1044, h=1044\n",
      "Image: 147.jpg, Bounding Box: x=183, y=937, w=1154, h=1154\n",
      "Image: 148.jpg, Bounding Box: x=40, y=633, w=1376, h=1376\n",
      "Image: 149.jpg, Bounding Box: x=470, y=1151, w=1366, h=1366\n",
      "Faces cropped and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#Using  Mediapipe detect the face in image and crop the face part only \n",
    "\n",
    "import os\n",
    "import cv2 \n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = r'D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\archive\\test\\Female'\n",
    "cropped_face_dir = r'D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\Images_aug\\Training\\female'\n",
    "\n",
    "if not os.path.exists(cropped_face_dir):\n",
    "    os.makedirs(cropped_face_dir)\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image at {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Initialize Face Detection\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        # Perform face detection\n",
    "        results = face_detection.process(image_rgb)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Get bounding box of the detected face\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = image.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                # Ensure the bounding box coordinates are within image dimensions\n",
    "                if x < 0: x = 0\n",
    "                if y < 0: y = 0\n",
    "                if x + w > iw: w = iw - x\n",
    "                if y + h > ih: h = ih - y\n",
    "                \n",
    "                # Debug information\n",
    "                print(f\"Image: {image_file}, Bounding Box: x={x}, y={y}, w={w}, h={h}\")\n",
    "\n",
    "                # Crop the face from the image\n",
    "                cropped_face = image[y:y+h, x:x+w]\n",
    "\n",
    "                if cropped_face.size == 0:\n",
    "                    print(f\"Failed to crop face from image: {image_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Generate unique filename\n",
    "                cropped_face_filename = f'cropped_face_{os.path.splitext(image_file)[0]}.jpg'\n",
    "                cropped_face_path = os.path.join(cropped_face_dir, cropped_face_filename)\n",
    "                \n",
    "                # Save the cropped face image\n",
    "                cv2.imwrite(cropped_face_path, cropped_face)\n",
    "\n",
    "                # Optionally, display the cropped face\n",
    "                # cv2.imshow('Cropped Face', cropped_face)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Faces cropped and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 0.jpg, Predicted class: 1\n",
      "Image: 1.jpg, Predicted class: 1\n",
      "Image: 10.jpg, Predicted class: 1\n",
      "Image: 100.jpg, Predicted class: 1\n",
      "Image: 101.jpg, Predicted class: 1\n",
      "Image: 102.jpg, Predicted class: 1\n",
      "Image: 103.jpg, Predicted class: 1\n",
      "Image: 104.jpg, Predicted class: 1\n",
      "Image: 105.jpg, Predicted class: 1\n",
      "Image: 106.jpg, Predicted class: 1\n",
      "Image: 107.jpg, Predicted class: 1\n",
      "Image: 108.jpg, Predicted class: 1\n",
      "Image: 109.jpg, Predicted class: 1\n",
      "Image: 11.jpg, Predicted class: 1\n",
      "Image: 12.jpg, Predicted class: 1\n",
      "Image: 13.jpg, Predicted class: 0\n",
      "Image: 14.jpg, Predicted class: 1\n",
      "Image: 15.jpg, Predicted class: 1\n",
      "Image: 16.jpg, Predicted class: 1\n",
      "Image: 17.jpg, Predicted class: 1\n",
      "Image: 18.jpg, Predicted class: 1\n",
      "Image: 19.jpg, Predicted class: 1\n",
      "Image: 2.jpg, Predicted class: 1\n",
      "Image: 21.jpg, Predicted class: 1\n",
      "Image: 22.jpg, Predicted class: 1\n",
      "Image: 23.jpg, Predicted class: 1\n",
      "Image: 24.jpg, Predicted class: 1\n",
      "Image: 25.jpg, Predicted class: 1\n",
      "Image: 26.jpg, Predicted class: 1\n",
      "Image: 27.jpg, Predicted class: 1\n",
      "Image: 28.jpg, Predicted class: 1\n",
      "Image: 29.jpg, Predicted class: 1\n",
      "Image: 3.jpg, Predicted class: 1\n",
      "Image: 30.jpg, Predicted class: 1\n",
      "Image: 31.jpg, Predicted class: 1\n",
      "Image: 32.jpg, Predicted class: 1\n",
      "Image: 33.jpg, Predicted class: 1\n",
      "Image: 34.jpg, Predicted class: 1\n",
      "Image: 35.jpg, Predicted class: 1\n",
      "Image: 36.jpg, Predicted class: 1\n",
      "Image: 37.jpg, Predicted class: 1\n",
      "Image: 38.jpg, Predicted class: 1\n",
      "Image: 39.jpg, Predicted class: 1\n",
      "Image: 4.jpg, Predicted class: 1\n",
      "Image: 40.jpg, Predicted class: 1\n",
      "Image: 41.jpg, Predicted class: 1\n",
      "Image: 42.jpg, Predicted class: 1\n",
      "Image: 43.jpg, Predicted class: 1\n",
      "Image: 44.jpg, Predicted class: 1\n",
      "Image: 45.jpg, Predicted class: 1\n",
      "Image: 47.jpg, Predicted class: 1\n",
      "Image: 48.jpg, Predicted class: 0\n",
      "Image: 49.jpg, Predicted class: 1\n",
      "Image: 5.jpg, Predicted class: 1\n",
      "Image: 50.jpg, Predicted class: 1\n",
      "Image: 51.jpg, Predicted class: 1\n",
      "Image: 52.jpg, Predicted class: 1\n",
      "Image: 53.jpg, Predicted class: 0\n",
      "Image: 54.jpg, Predicted class: 1\n",
      "Image: 55.jpg, Predicted class: 1\n",
      "Image: 56.jpg, Predicted class: 1\n",
      "Image: 57.jpg, Predicted class: 1\n",
      "Image: 58.jpg, Predicted class: 1\n",
      "Image: 59.jpg, Predicted class: 1\n",
      "Image: 6.jpg, Predicted class: 1\n",
      "Image: 60.jpg, Predicted class: 1\n",
      "Image: 61.jpg, Predicted class: 1\n",
      "Image: 62.jpg, Predicted class: 1\n",
      "Image: 63.jpg, Predicted class: 1\n",
      "Image: 64.jpg, Predicted class: 1\n",
      "Image: 65.jpg, Predicted class: 1\n",
      "Image: 66.jpg, Predicted class: 1\n",
      "Image: 67.jpg, Predicted class: 1\n",
      "Image: 68.jpg, Predicted class: 1\n",
      "Image: 69.jpg, Predicted class: 1\n",
      "Image: 7.jpg, Predicted class: 1\n",
      "Image: 70.jpg, Predicted class: 1\n",
      "Image: 71.jpg, Predicted class: 1\n",
      "Image: 72.jpg, Predicted class: 1\n",
      "Image: 73.jpg, Predicted class: 1\n",
      "Image: 74.jpg, Predicted class: 1\n",
      "Image: 75.jpg, Predicted class: 1\n",
      "Image: 76.jpg, Predicted class: 1\n",
      "Image: 77.jpg, Predicted class: 1\n",
      "Image: 78.jpg, Predicted class: 1\n",
      "Image: 79.jpg, Predicted class: 1\n",
      "Image: 8.jpg, Predicted class: 1\n",
      "Image: 80.jpg, Predicted class: 1\n",
      "Image: 81.jpg, Predicted class: 1\n",
      "Image: 82.jpg, Predicted class: 1\n",
      "Image: 83.jpg, Predicted class: 1\n",
      "Image: 84.jpg, Predicted class: 1\n",
      "Image: 85.jpg, Predicted class: 1\n",
      "Image: 86.jpg, Predicted class: 1\n",
      "Image: 87.jpg, Predicted class: 1\n",
      "Image: 88.jpg, Predicted class: 1\n",
      "Image: 89.jpg, Predicted class: 1\n",
      "Image: 9.jpg, Predicted class: 1\n",
      "Image: 90.jpg, Predicted class: 1\n",
      "Image: 91.jpg, Predicted class: 1\n",
      "Image: 92.jpg, Predicted class: 1\n",
      "Image: 93.jpg, Predicted class: 1\n",
      "Image: 94.jpg, Predicted class: 1\n",
      "Image: 95.jpg, Predicted class: 1\n",
      "Image: 96.jpg, Predicted class: 1\n",
      "Image: 97.jpg, Predicted class: 1\n",
      "Image: 98.jpg, Predicted class: 1\n",
      "Image: 99.jpg, Predicted class: 1\n",
      "Faces cropped and classified successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the Vision Transformer model\n",
    "model = create_model('vit_base_patch16_224', pretrained=False)\n",
    "num_classes = 2  # Update this to match the number of classes\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(\"best_vit_model_dataset2.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to predict the class of an image tensor\n",
    "def predict_image_tensor(image_tensor):\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = r'D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\Images_aug\\Validation'\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image at {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Initialize Face Detection\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        # Perform face detection\n",
    "        results = face_detection.process(image_rgb)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Get bounding box of the detected face\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = image.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                # Ensure the bounding box coordinates are within image dimensions\n",
    "                if x < 0: x = 0\n",
    "                if y < 0: y = 0\n",
    "                if x + w > iw: w = iw - x\n",
    "                if y + h > ih: h = ih - y\n",
    "                \n",
    "                # Crop the face from the image\n",
    "                cropped_face = image[y:y+h, x:x+w]\n",
    "                \n",
    "                if cropped_face.size == 0:\n",
    "                    print(f\"Failed to crop face from image: {image_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert cropped face to PIL image\n",
    "                cropped_face_pil = Image.fromarray(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                # Apply transformation\n",
    "                cropped_face_tensor = transform(cropped_face_pil)\n",
    "                \n",
    "                # Predict the class of the cropped face\n",
    "                predicted_class = predict_image_tensor(cropped_face_tensor)\n",
    "                print(f\"Image: {image_file}, Predicted class: {predicted_class}\")\n",
    "\n",
    "print(\"Faces cropped and classified successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9635258358662614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Male       0.00      0.00      0.00         0\n",
      "      Female       1.00      0.96      0.98       329\n",
      "\n",
      "    accuracy                           0.96       329\n",
      "   macro avg       0.50      0.48      0.49       329\n",
      "weighted avg       1.00      0.96      0.98       329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'y_true' parameter of confusion_matrix must be an array-like. Got 1 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(true_labels, predicted_labels, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFemale\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n\u001b[1;32m--> 117\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(cm)\n",
      "File \u001b[1;32md:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32md:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\.facedetection\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'y_true' parameter of confusion_matrix must be an array-like. Got 1 instead."
     ]
    }
   ],
   "source": [
    "# This code is to check the \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the Vision Transformer model\n",
    "model = create_model('vit_base_patch16_224', pretrained=False)\n",
    "num_classes = 2  # Update this to match the number of classes\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(\"best_vit_model_dataset2.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to predict the class of an image tensor\n",
    "def predict_image_tensor(image_tensor):\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = r'D:\\New folder\\Pavan Sai\\Data Science\\OpenCV\\Face Detection\\Images_aug\\Validation\\male'\n",
    "true_labels = []  # List to store true labels\n",
    "predicted_labels = []  # List to store predicted labels\n",
    "\n",
    "# Function to extract true label from the filename or directory structure\n",
    "# Modify this function based on how your true labels are encoded in the filenames or directory structure\n",
    "def get_true_label(image_file):\n",
    "    # Example: Assuming the directory name is the label\n",
    "    label = os.path.basename(os.path.dirname(image_file))\n",
    "    return 0 if label == 'men' else 1  # Replace with actual label extraction logic\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image at {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the image to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Initialize Face Detection\n",
    "    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n",
    "        # Perform face detection\n",
    "        results = face_detection.process(image_rgb)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Get bounding box of the detected face\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = image.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                # Ensure the bounding box coordinates are within image dimensions\n",
    "                if x < 0: x = 0\n",
    "                if y < 0: y = 0\n",
    "                if x + w > iw: w = iw - x\n",
    "                if y + h > ih: h = ih - y\n",
    "                \n",
    "                # Crop the face from the image\n",
    "                cropped_face = image[y:y+h, x:x+w]\n",
    "                \n",
    "                if cropped_face.size == 0:\n",
    "                    print(f\"Failed to crop face from image: {image_file}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert cropped face to PIL image\n",
    "                cropped_face_pil = Image.fromarray(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB))\n",
    "                \n",
    "                # Apply transformation\n",
    "                cropped_face_tensor = transform(cropped_face_pil)\n",
    "                \n",
    "                # Predict the class of the cropped face\n",
    "                predicted_class = predict_image_tensor(cropped_face_tensor)\n",
    "                predicted_labels.append(predicted_class)\n",
    "                \n",
    "                # Extract true label\n",
    "                true_label = get_true_label(image_path)\n",
    "                true_labels.append(true_label)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['Male', 'Female'])\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(true_label, predicted_labels)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genderdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
